# Plan: Per-Speaker TTS Generation with Audio Combining

## Summary

Refactor the TTS generation system from single multi-speaker API calls to per-segment calls (max 2 speakers: character + narrator), enabling support for unlimited characters and better voice tuning through inline emotional markers.

## Key Changes

1. **New audio script format** - Simplified frontmatter with speaker voice profiles, inline `<emotion:>` markers, no separate DIRECTOR'S NOTES/THE SCENE sections
2. **Per-segment TTS generation** - Parse transcript into segments, batch narrator with character, generate audio per-segment with parallel execution
3. **Audio combining** - Concatenate segments with 300ms pauses and 50ms normalized silence buffers
4. **Command updates** - Update kidstory commands with LLM guidance for emotional markers and character profile templates

---

## Phase 1: New Audio Script Format

### 1.1 New Frontmatter Schema

**File:** `generate_audio.py` (update `SpeakerConfig` and `AudioScript` dataclasses)

```yaml
---
stageUuid: "stage-entering-forest"
chapterRef: "02-entering-forest"
locale: "en-US"
speakers:
  - name: Narrator
    voice: Sulafat
    profile: "Warm storyteller, like a beloved aunt, uses vocal smiles on beautiful moments"
  - name: Emma
    voice: Leda
    profile: "8-year-old girl, curious and brave, speaks with wonder and excitement"
  - name: Dragon
    voice: Fenrir
    profile: "Ancient but friendly dragon, deep rumbling voice, wise and patient"
---
```

### 1.2 New Transcript Format

Body is purely transcript with inline `<emotion:>` markers:

```markdown
**Narrator:** Emma took a deep breath and stepped onto the mossy path.

**Narrator:** <emotion: mysterious, soft> The forest whispered secrets only she could hear.

**Emma:** <emotion: nervous, quiet> Hello? Is anyone there?

**Narrator:** A shadow moved between the trees. Emma's heart raced.

**Emma:** <emotion: frightened, trembling> What was that?!

**Dragon:** <emotion: gentle, reassuring> Do not be afraid, little one.
```

### 1.3 Updated Dataclasses

```python
@dataclass
class SpeakerConfig:
    """Configuration for a single speaker."""
    name: str
    voice: str = DEFAULT_VOICE
    profile: str = ""  # NEW: Voice profile description

@dataclass 
class Segment:
    """A single speaker segment with optional emotion."""
    speaker: str
    text: str
    emotion: str = ""  # From <emotion:> marker or narrator context

@dataclass
class AudioScript:
    """Parsed audio script from markdown file."""
    stage_uuid: str
    chapter_ref: str = ""
    locale: str = "en-US"
    speaker_configs: list[SpeakerConfig] = field(default_factory=list)
    segments: list[Segment] = field(default_factory=list)  # NEW: Parsed segments
```

---

## Phase 2: Transcript Parsing

### 2.1 Segment Parser

**New function:** `parse_transcript(content: str) -> list[Segment]`

Parse the transcript body into ordered segments:

1. Split by `**Speaker:**` pattern
2. Extract `<emotion:>` markers from each segment
3. Handle multiple emotions in one block → split into multiple segments
4. Track narrator context for character emotion inheritance

**Algorithm:**
```python
def parse_transcript(content: str, speaker_configs: list[SpeakerConfig]) -> list[Segment]:
    segments = []
    narrator_context = ""  # Last narrator emotional context
    
    # Regex: **Speaker:** followed by content until next **Speaker:** or end
    pattern = r'\*\*(\w+):\*\*\s*(.*?)(?=\*\*\w+:\*\*|$)'
    
    for match in re.finditer(pattern, content, re.DOTALL):
        speaker = match.group(1)
        text = match.group(2).strip()
        
        # Check for multiple <emotion:> markers → split
        emotion_splits = split_by_emotions(text)
        
        for emotion, segment_text in emotion_splits:
            # If no emotion and speaker != Narrator, inherit from narrator context
            if not emotion and speaker != "Narrator" and narrator_context:
                emotion = narrator_context
                narrator_context = ""  # Clear after use (merged)
            
            # If Narrator, extract context for next character
            if speaker == "Narrator":
                narrator_context = extract_emotional_context(segment_text)
            
            segments.append(Segment(speaker=speaker, text=segment_text, emotion=emotion))
    
    return segments
```

### 2.2 Emotion Marker Handling

**New function:** `split_by_emotions(text: str) -> list[tuple[str, str]]`

```python
def split_by_emotions(text: str) -> list[tuple[str, str]]:
    """Split text by <emotion:> markers into (emotion, text) pairs."""
    # Pattern: <emotion: ...> followed by text
    pattern = r'<emotion:\s*([^>]+)>\s*'
    
    parts = re.split(pattern, text)
    # parts = [pre_text, emotion1, text1, emotion2, text2, ...]
    
    results = []
    if parts[0].strip():  # Text before first emotion marker
        results.append(("", parts[0].strip()))
    
    for i in range(1, len(parts), 2):
        emotion = parts[i].strip()
        segment_text = parts[i + 1].strip() if i + 1 < len(parts) else ""
        if segment_text:
            results.append((emotion, segment_text))
    
    return results if results else [("", text)]
```

### 2.3 Narrator Context Extraction

**New function:** `extract_emotional_context(narrator_text: str) -> str`

Extract implicit emotional cues from narrator text for the next character:

```python
def extract_emotional_context(narrator_text: str) -> str:
    """Extract emotional context from narrator text for next character."""
    # Look for patterns like "she whispered", "he shouted", "said softly"
    patterns = [
        r'(whispered|murmured|muttered)\s*(urgently|softly|nervously)?',
        r'(shouted|yelled|exclaimed)\s*(excitedly|angrily)?',
        r'(said|spoke)\s+(softly|quietly|firmly|gently|nervously)',
        r'voice\s+(trembling|shaking|rising|breaking)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, narrator_text, re.IGNORECASE)
        if match:
            return match.group(0)
    
    return ""
```

---

## Phase 3: Segment Batching

### 3.1 Batching Strategy

**New function:** `batch_segments(segments: list[Segment]) -> list[SegmentBatch]`

Batch segments according to rules:
- Option B: Each character segment with its preceding narrator
- Narrator-only batches with following character if exists
- Character-to-character = separate single-speaker calls

```python
@dataclass
class SegmentBatch:
    """A batch of segments for a single TTS call (max 2 speakers)."""
    segments: list[Segment]
    speakers: list[str]  # Unique speakers in this batch (max 2)

def batch_segments(segments: list[Segment]) -> list[SegmentBatch]:
    batches = []
    pending_narrator = []
    
    for segment in segments:
        if segment.speaker == "Narrator":
            pending_narrator.append(segment)
        else:
            # Character segment - batch with pending narrator
            if pending_narrator:
                batch = SegmentBatch(
                    segments=pending_narrator + [segment],
                    speakers=["Narrator", segment.speaker]
                )
                pending_narrator = []
            else:
                # No preceding narrator - single speaker
                batch = SegmentBatch(
                    segments=[segment],
                    speakers=[segment.speaker]
                )
            batches.append(batch)
    
    # Handle trailing narrator segments
    if pending_narrator:
        # Check if there's a next batch to merge with
        # If not, create standalone narrator batch
        batches.append(SegmentBatch(
            segments=pending_narrator,
            speakers=["Narrator"]
        ))
    
    return batches
```

---

## Phase 4: TTS Generation Refactoring

### 4.1 Per-Batch TTS Generation

**New function:** `generate_batch_audio(batch: SegmentBatch, ...) -> bytes`

```python
def generate_batch_audio(
    client: genai.Client,
    batch: SegmentBatch,
    speaker_configs: dict[str, SpeakerConfig],
    tts_model: str,
    retry_count: int = 3
) -> bytes:
    """Generate audio for a segment batch with retry logic."""
    
    # Build transcript for this batch
    transcript_parts = []
    for segment in batch.segments:
        # Build prompt with emotion and profile
        config = speaker_configs[segment.speaker]
        
        if segment.emotion:
            # Prepend emotion as style guidance
            style_prefix = f"[Voice: {config.profile}. Emotion: {segment.emotion}]"
        else:
            style_prefix = f"[Voice: {config.profile}]"
        
        transcript_parts.append(f"**{segment.speaker}:** {style_prefix} {segment.text}")
    
    transcript = "\n\n".join(transcript_parts)
    
    # Build speech config (single or multi-speaker)
    if len(batch.speakers) == 1:
        speech_config = build_single_speaker_config(speaker_configs[batch.speakers[0]])
    else:
        speech_config = build_multi_speaker_config(
            [speaker_configs[s] for s in batch.speakers]
        )
    
    # Generate with retry
    for attempt in range(retry_count):
        try:
            response = client.models.generate_content(
                model=tts_model,
                contents=transcript,
                config=types.GenerateContentConfig(
                    response_modalities=["AUDIO"],
                    speech_config=speech_config,
                )
            )
            
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    return part.inline_data.data
            
            raise RuntimeError("No audio data in response")
            
        except Exception as e:
            if attempt < retry_count - 1:
                logging.warning(f"Batch generation failed (attempt {attempt + 1}): {e}")
                time.sleep(1)  # Brief pause before retry
            else:
                raise
```

### 4.2 Parallel Batch Processing

**New function:** `generate_all_batches_parallel(...) -> list[bytes]`

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

MAX_CONCURRENT_TTS = 5  # Configurable concurrency

def generate_all_batches_parallel(
    client: genai.Client,
    batches: list[SegmentBatch],
    speaker_configs: dict[str, SpeakerConfig],
    tts_model: str,
    progress_callback: Callable[[int, int], None] = None
) -> list[bytes]:
    """Generate audio for all batches with parallel execution."""
    
    results = [None] * len(batches)
    
    def generate_one(index: int) -> tuple[int, bytes]:
        audio = generate_batch_audio(
            client, batches[index], speaker_configs, tts_model
        )
        if progress_callback:
            progress_callback(index + 1, len(batches))
        return (index, audio)
    
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TTS) as executor:
        futures = [executor.submit(generate_one, i) for i in range(len(batches))]
        
        for future in futures:
            index, audio = future.result()
            results[index] = audio
    
    return results
```

---

## Phase 5: Audio Combining

### 5.1 Silence Normalization

**New function:** `normalize_segment_audio(audio: AudioSegment, buffer_ms: int = 50) -> AudioSegment`

```python
SILENCE_BUFFER_MS = 50  # Normalized silence at segment boundaries

def normalize_segment_audio(audio: AudioSegment, buffer_ms: int = SILENCE_BUFFER_MS) -> AudioSegment:
    """Normalize leading/trailing silence to consistent duration."""
    
    # Detect silence threshold (dBFS)
    silence_threshold = audio.dBFS - 16
    
    # Find first non-silent sample
    start_trim = detect_leading_silence(audio, silence_threshold)
    end_trim = detect_leading_silence(audio.reverse(), silence_threshold)
    
    # Trim silence
    trimmed = audio[start_trim:len(audio) - end_trim]
    
    # Add normalized silence buffer
    silence = AudioSegment.silent(duration=buffer_ms)
    return silence + trimmed + silence
```

### 5.2 Audio Concatenation

**Updated function:** `concatenate_segment_audio(...)`

```python
INTER_SEGMENT_PAUSE_MS = 300

def concatenate_segment_audio(
    audio_segments: list[bytes],
    output_path: Path,
    pause_ms: int = INTER_SEGMENT_PAUSE_MS,
    buffer_ms: int = SILENCE_BUFFER_MS
) -> None:
    """Concatenate segment audio files with pauses."""
    
    # Convert PCM to AudioSegment and normalize
    segments = []
    for pcm_data in audio_segments:
        audio = pcm_to_audio_segment(pcm_data)
        normalized = normalize_segment_audio(audio, buffer_ms)
        segments.append(normalized)
    
    # Combine with pauses
    pause = AudioSegment.silent(duration=pause_ms)
    combined = segments[0]
    for segment in segments[1:]:
        combined += pause + segment
    
    # Ensure format and export
    combined = combined.set_frame_rate(TARGET_SAMPLE_RATE)
    combined = combined.set_channels(TARGET_CHANNELS)
    
    # Export and strip ID3
    mp3_buffer = io.BytesIO()
    combined.export(mp3_buffer, format="mp3", parameters=["-id3v2_version", "0"])
    mp3_data = strip_id3_tags(mp3_buffer.getvalue())
    
    output_path.write_bytes(mp3_data)
```

---

## Phase 6: Progress and Debug Output

### 6.1 Progress Bar

```python
def print_progress(current: int, total: int) -> None:
    """Print progress bar for segment generation."""
    bar_width = 40
    progress = current / total
    filled = int(bar_width * progress)
    bar = '=' * filled + '-' * (bar_width - filled)
    print(f"\rGenerating segments: [{bar}] {current}/{total}", end='', flush=True)
    if current == total:
        print()  # Newline at completion
```

### 6.2 Debug Mode Enhancements

When `--debug` is passed:

1. **Dump parsed segments:**
   ```
   Parsed 12 segments:
     [1] Narrator: "Emma took a deep breath..." (emotion: none)
     [2] Narrator: "The forest whispered..." (emotion: mysterious, soft)
     [3] Emma: "Hello? Is anyone there?" (emotion: nervous, quiet)
     ...
   ```

2. **Save intermediate files:**
   - `{output_dir}/debug/segment_001.mp3`
   - `{output_dir}/debug/segment_002.mp3`
   - `{output_dir}/debug/segments.json` (metadata)

---

## Phase 7: Example Script Migration

### 7.1 Files to Update

1. `examples/sample-audio-script.md`
2. `examples/sample-audio-script-fr.md`
3. `examples/dragon-lair-chapter.md`
4. `examples/dungeon-entrance.md`
5. `examples/simple-audio-script.md`

### 7.2 Migration Example

**Before (dungeon-entrance.md):**
```markdown
---
stageUuid: "stage-dungeon-entrance"
chapterRef: "01-dungeon-entrance"
speakers: ["Narrator", "Thorin", "Elara", "Finn"]
locale: "en-US"
---

# AUDIO PROFILE: Adventure Party
## "The Seekers of the Lost Gate"

## THE SCENE: Ancient Forest Ruins
Late afternoon sunlight filters through...

### DIRECTOR'S NOTES
Style:
* Light and adventurous...
...

## TRANSCRIPT

**Narrator:** The late afternoon sun painted everything...

## TTS CONFIGURATION
```json
{"speakerConfigs": [...]}
```
```

**After:**
```markdown
---
stageUuid: "stage-dungeon-entrance"
chapterRef: "01-dungeon-entrance"
locale: "en-US"
speakers:
  - name: Narrator
    voice: Sulafat
    profile: "Warm storyteller, paints vivid scenes, slows for atmospheric moments"
  - name: Thorin
    voice: Algenib
    profile: "Gruff dwarf warrior, 40 years adventuring, warmhearted beneath the grumbles"
  - name: Elara
    voice: Despina
    profile: "Elven mage, wise with dry humor, patient and mysterious"
  - name: Finn
    voice: Puck
    profile: "Young enthusiastic human, curious and occasionally reckless, full of wonder"
---

**Narrator:** <emotion: warm, atmospheric> The late afternoon sun painted everything in shades of gold and amber... as three unlikely companions made their way through the ruins.

**Finn:** <emotion: excited, insistent> I'm telling you, the entrance has to be here somewhere!

**Thorin:** <emotion: grumbling, skeptical> Aye, and he also said the ale was free if we bought his map. Turned out that was a lie too.

**Finn:** <emotion: defensive> That's... that's not the same thing at all!

**Thorin:** <emotion: complaining> All I'm saying is, we've been walking through this overgrown mess for three hours.

**Elara:** <emotion: calm, mysterious> Patience, old friend. Magic leaves traces... and something here is definitely whispering.

**Finn:** <emotion: nervous> Whispering? Good whispering or bad whispering?

**Elara:** <emotion: enigmatic> The kind that says... we are not the first to seek this place.

**Thorin:** <emotion: sarcastic> Oh, wonderful. Cryptic elf wisdom. That's helpful.

**Narrator:** Elara paused beside a moss-covered pillar. The elven mage closed her eyes...

**Narrator:** <emotion: reverent, soft> ...and when she opened them, they held a faint silver glow.
```

---

## Phase 8: Command Updates

### 8.1 Update kidstory-new.md

Add section for emotional marker guidance:

```markdown
## Emotional Markers for TTS

When generating audio scripts, use inline `<emotion:>` markers to guide voice performance:

### Marker Format
```
**Speaker:** <emotion: descriptor1, descriptor2> "Dialogue text"
```

### Common Emotion Descriptors
- **Volume:** whispered, soft, loud, shouting
- **Pace:** rushed, slow, hesitant, deliberate
- **Feeling:** happy, sad, scared, excited, nervous, angry, calm, mysterious
- **Quality:** trembling, firm, gentle, harsh, playful, serious

### Examples
```
**Emma:** <emotion: nervous, quiet> "Is someone there?"
**Dragon:** <emotion: gentle, rumbling> "Do not fear, little one."
**Narrator:** <emotion: tense, hushed> The door creaked open slowly...
```

### Guidelines
- Use 1-3 descriptors per marker
- Place marker immediately after speaker label
- Narrator can have emotions too (affects narration tone)
- If no marker, voice uses character's baseline profile
```

### 8.2 Character Profile Templates

Add to kidstory-new.md:

```markdown
## Character Voice Profile Templates

When defining speaker profiles, use these archetypes as starting points:

### Young Child (5-8 years)
- **Profile:** "Young child, speaks with wonder and curiosity, simple vocabulary, enthusiastic"
- **Suggested voices:** Leda (F), Puck (M)

### Brave Young Hero (8-12 years)
- **Profile:** "Determined young hero, curious and brave, speaks clearly with growing confidence"
- **Suggested voices:** Kore (F), Achird (M)

### Wise Mentor/Elder
- **Profile:** "Wise elder, patient and kind, speaks slowly with weight and warmth"
- **Suggested voices:** Gacrux (F), Charon (M), Sadaltager (M)

### Playful Sidekick/Animal Friend
- **Profile:** "Playful companion, energetic and loyal, quick wit, expressive reactions"
- **Suggested voices:** Laomedeia (F), Puck (M), Sadachbia (M)

### Mysterious/Magical Being
- **Profile:** "Enigmatic presence, speaks with otherworldly quality, hints at ancient knowledge"
- **Suggested voices:** Zephyr (F), Enceladus (M)

### Friendly Monster/Creature
- **Profile:** "Large but gentle creature, deep voice, surprisingly kind, slightly formal"
- **Suggested voices:** Fenrir (M), Algenib (M)

### Warm Parent/Guardian
- **Profile:** "Loving caregiver, warm and reassuring, protective, gentle encouragement"
- **Suggested voices:** Sulafat (F), Vindemiatrix (F), Umbriel (M)

### Story Narrator
- **Profile:** "Warm storyteller, engaging and expressive, guides listener through the tale"
- **Suggested voices:** Sulafat (F), Charon (M)
```

### 8.3 Validation Rule

Add speaker validation to kidstory commands:

```markdown
## Validation: Speaker Consistency

Before generating audio, verify:
- All speakers in transcript have matching entry in frontmatter `speakers` list
- No undefined speakers appear in `**Speaker:**` tags
- Each speaker has a voice and profile defined

Example error:
```
ERROR: Speaker "Wizard" found in transcript but not defined in frontmatter.
Add to frontmatter:
  - name: Wizard
    voice: Enceladus
    profile: "Description of voice and character"
```
```

### 8.4 Segment Length Constraint

Add to content guidelines:

```markdown
## Audio Segment Length Guidelines

To ensure optimal TTS quality:
- Keep individual character speeches under 500 words
- Break long monologues with narrator interjections
- Maximum chapter transcript: 5000 characters (enforced)

If a chapter exceeds limits:
1. Split into multiple chapters, OR
2. Add narrator breaks to create natural segment boundaries
```

---

## Phase 9: File Changes Summary

### Files to Modify

| File | Changes |
|------|---------|
| `generate_audio.py` | Major refactor: new parsing, batching, parallel generation, audio combining |
| `.opencode/commands/kidstory-new.md` | Add emotional marker guidance, profile templates, validation rules |
| `.opencode/commands/kidstory-export.md` | Update audio generation section for new format |
| `examples/sample-audio-script.md` | Migrate to new format |
| `examples/sample-audio-script-fr.md` | Migrate to new format |
| `examples/dragon-lair-chapter.md` | Migrate to new format |
| `examples/dungeon-entrance.md` | Migrate to new format |
| `examples/simple-audio-script.md` | Migrate to new format |
| `docs/voice-guide.md` | Add emotional markers section, update examples |

### New Constants

```python
# generate_audio.py
SILENCE_BUFFER_MS = 50        # Normalized silence at segment edges
INTER_SEGMENT_PAUSE_MS = 300  # Pause between segments
MAX_CONCURRENT_TTS = 5        # Parallel TTS calls
MAX_RETRIES = 3               # Retry count per segment
```

---

## Phase 10: Testing & Verification

### 10.1 Unit Tests

1. **Segment parsing tests:**
   - Parse simple transcript
   - Parse with emotion markers
   - Parse with multiple emotions in one block
   - Narrator context inheritance

2. **Batching tests:**
   - Narrator + character batching
   - Character-to-character (separate batches)
   - Trailing narrator handling

3. **Emotion extraction tests:**
   - "she whispered" → "whispered"
   - "said softly" → "said softly"
   - No emotional context → empty string

### 10.2 Integration Tests

1. Generate audio from migrated `sample-audio-script.md`
2. Generate audio from `dungeon-entrance.md` (4 speakers)
3. Verify output format (44100Hz, mono, no ID3)
4. Test retry on simulated failure

### 10.3 Manual Verification

1. **Listen to output audio:**
   - Verify all speakers are present
   - Check emotional variation
   - Confirm natural pacing with 300ms pauses

2. **Compare with old format:**
   - Generate same script with old multi-speaker approach
   - Compare quality and timing

---

## Implementation Order

1. **Phase 1-2:** New dataclasses and transcript parsing
2. **Phase 3:** Segment batching logic
3. **Phase 4:** Per-batch TTS generation with retry
4. **Phase 5:** Audio combining with silence normalization
5. **Phase 6:** Progress bar and debug output
6. **Phase 7:** Migrate example scripts
7. **Phase 8:** Update kidstory commands
8. **Phase 9:** Update documentation
9. **Phase 10:** Testing and verification
